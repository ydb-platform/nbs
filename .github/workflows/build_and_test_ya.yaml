name: Build and test

on:
  workflow_call:
    inputs:
      build_target:
        type: string
        default: "cloud/blockstore/apps/,cloud/filestore/apps/,cloud/disk_manager/,cloud/tasks/,cloud/storage/"
        description: "limit build to specific target"
      test_target:
        type: string
        default: "cloud/blockstore/,cloud/filestore/,cloud/disk_manager/,cloud/tasks/,cloud/storage/"
        description: "limit test to specific target"
      build_preset:
        type: string
      runner_kind:
        type: string
        required: true
        description: "self-hosted or provisioned"
      runner_label:
        type: string
        default: "none"
        description: "runner label"
      runner_flavor:
        type: string
        default: "none"
        description: "runner flavor"
      run_build:
        type: boolean
        default: true
        description: "run build"
      run_tests:
        type: boolean
        default: true
        description: "run tests"
      sleep_after_tests:
        type: string
        default: "0"
        description: "Amount of time to sleep after tests"
      test_size:
        type: string
        default: "small,medium,large"
      test_type:
        type: string
        default: "unittest,clang_tidy,gtest,py3test,py2test,pytest,flake8,black,py2_flake8,go_test,gofmt"
      folder_prefix:
        type: string
        default: "nebius-"
      cache_update_build:
        type: boolean
        default: false
        description: "Update remote build cache"
      cache_update_tests:
        type: boolean
        default: false
        description: "Update remote build cache"
      upload_ya_dir:
        type: string
        default: "no"
        description: "Upload ya dir to s3"
      clean_ya_dir:
        type: string
        default: "no"
        description: "Clean ya dir from image before building"
      use_network_cache:
        type: string
        default: "yes"
        description: "Use network cache"
      truncate_enabled:
        type: string
        default: "yes"
        description: "Truncate enabled"
      number_of_retries:
        type: string
        default: "1"
        description: "Number of retries for tests"

jobs:
  calculate-runs-on:
    name: Calculate runs-on
    runs-on: [ self-hosted, "self-hosted", "runner_light" ]
    outputs:
      runs-on: ${{ steps.set-runs-on.outputs.LABELS_LIST }}
    steps:
      - name: set up runs on
        id: set-runs-on
        shell: bash --noprofile --norc -eo pipefail -x {0}
        env:
          RUNNER_LABEL: ${{ inputs.runner_label }}
          RUNNER_FLAVOR: ${{ inputs.runner_flavor }}
          RUNNER_KIND: ${{ inputs.runner_kind }}
        run: |
          LABELS_LIST='[]'
          LABELS_LIST=$(echo "${LABELS_LIST}" | jq -c --arg gh_label "self-hosted" '. + [$gh_label]')
          LABELS_LIST=$(echo "${LABELS_LIST}" | jq -c --arg gh_label "runner_${RUNNER_FLAVOR}" '. + [$gh_label]')
          if [ "${RUNNER_LABEL}" != "none" ]; then
            LABELS_LIST=$(echo "${LABELS_LIST}" | jq  -c --arg gh_label "${RUNNER_LABEL}" '. + [$gh_label]')
          fi
          echo "LABELS_LIST=${LABELS_LIST}" | tee -a "$GITHUB_OUTPUT"

  build-and-test:
    name: Build and test ${{ inputs.build_preset }}
    runs-on: ${{ fromJson(needs.calculate-runs-on.outputs.runs-on) }}
    timeout-minutes: 420
    needs: calculate-runs-on
    outputs:
      # outcome failure and test_failed is null -> vm is broken
      # failure and test_failed is 'no' -> tests failed, but vm is ok
      # failure and test_failed is 'yes' -> tests failed, vm could be broken
      test_failed: ${{ steps.test.outputs.test_failed || 'null' }}
      test_run_outcome: ${{ steps.test.outcome }}
    steps:
    - name: Prepare workspace
      shell: bash --noprofile --norc -eo pipefail -x {0}
      run: |
        chown -R root:root $GITHUB_WORKSPACE
    - name: Checkout PR
      uses: actions/checkout@v4
      if: github.event.pull_request.head.sha != ''
      with:
        submodules: true
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: ${{ !contains(github.event.pull_request.labels.*.name, 'rebase') && 1 || 0 }}
    - name: Rebase PR
      if: ${{ github.event.pull_request.head.sha != '' && contains(github.event.pull_request.labels.*.name, 'rebase') }}
      shell: bash
      run: |
        git config user.email "robot-nbs@nebius.com"
        git config user.name "Robot NBS"
        git fetch origin ${{ github.event.pull_request.base.ref }}
        git rebase origin/${{ github.event.pull_request.base.ref }}
    - name: Checkout
      uses: actions/checkout@v4
      if: github.event.pull_request.head.sha == ''
      with:
        submodules: true
        set-safe-directory: '*'

    - name: set up nebius cli
      id: cli
      uses: ./.github/actions/nebius_cli
      with:
        sa_json: ${{ secrets.NEW_NEBIUS_SA_JSON_CREDENTIALS }}
    - name: set up gh cli
      id: gh
      uses: ./.github/actions/gh_cli
    - name: get runner info
      id: get-runner
      shell: bash --noprofile --norc -eo pipefail -x {0}
      env:
        RUNNER_NAME: ${{ runner.name }}
      run: |
        nebius compute instance get --id "${RUNNER_NAME}" --format json > instance.json
        LABEL=$(jq -r '.metadata.labels."runner-label"' instance.json)
        RUNNER_FLAVOR=$(jq -r '.metadata.labels."runner-flavor"' instance.json)
        EXTERNAL_IPV4=$(jq -r '.status.network_interfaces[0].public_ip_address.address' instance.json | sed -e 's/\/32//g')
        LOCAL_IPV4=$(jq -r '.status.network_interfaces[0].ip_address.address' instance.json | sed -e 's/\/32//g')
        VM_PRESET=$(jq -r '.spec.resources.preset' instance.json)
        {
          echo "LABEL=${LABEL}"
          echo "RUNNER_FLAVOR=${RUNNER_FLAVOR}"
          echo "INSTANCE_ID=${INSTANCE_ID}"
          echo "RUNNER_IPV4=${EXTERNAL_IPV4}"
          echo "RUNNER_LOCAL_IPV4=${LOCAL_IPV4}"
          echo "VM_PRESET=${VM_PRESET}"
        } | tee -a "$GITHUB_OUTPUT"

    - name: Calculate threads
      id: calculate-threads
      if: always()
      uses: ./.github/actions/nebius_threads_calculator
      with:
        vm_preset: ${{ steps.get-runner.outputs.VM_PRESET }}
        tests_size: ${{ inputs.test_size }}

    - name: Prepare s3cmd
      uses: ./.github/actions/s3cmd
      with:
        s3_bucket: ${{ vars.AWS_BUCKET }}
        s3_endpoint: ${{ vars.NEBIUS_AWS_ENDPOINT }}
        s3_website_suffix: ${{ vars.NEBIUS_AWS_WEBSITE_SUFFIX }}
        s3_key_id: ${{ secrets.NEBIUS_AWS_ACCESS_KEY_ID }}
        s3_key_secret: ${{ secrets.NEBIUS_AWS_SECRET_ACCESS_KEY }}
        folder_prefix: nebius-
        build_preset: ${{ inputs.build_preset }}
        install: false

    - name: install unattended-upgrades
      shell: bash --noprofile --norc -eo pipefail -x {0}
      run: |
        [ -f /etc/apt/apt.conf.d/50unattended-upgrades ] && echo "unattended-upgrades already installed" && exit 0
        sudo apt-get -o Dpkg::Lock::Timeout=600 update
        sudo apt-get install -y unattended-upgrades

    - name: Build
      uses: ./.github/actions/build
      if: inputs.run_build
      with:
        build_target: ${{ inputs.build_target }}
        build_preset: ${{ inputs.build_preset }}
        cache_update: ${{ inputs.cache_update_build }}
        bazel_remote_uri: ${{ vars.NEBIUS_BAZEL_REMOTE_CACHE_URL }}
        bazel_remote_username: ${{ secrets.REMOTE_CACHE_USERNAME }}
        bazel_remote_password: ${{ secrets.REMOTE_CACHE_PASSWORD }}
        build_threads: ${{ steps.calculate-threads.outputs.build_threads }}
        link_threads: ${{ steps.calculate-threads.outputs.link_threads }}
        clean_ya_dir: ${{ inputs.clean_ya_dir }}
        use_network_cache: ${{ inputs.use_network_cache }}

    - name: Run tests
      uses: ./.github/actions/test
      id: test
      if: inputs.run_tests
      with:
        test_target: ${{ inputs.test_target }}
        build_preset: ${{ inputs.build_preset }}
        test_size: ${{ inputs.test_size }}
        test_type: ${{ inputs.test_type }}
        cache_update: ${{ inputs.cache_update_tests }}
        bazel_remote_uri: ${{ vars.NEBIUS_BAZEL_REMOTE_CACHE_URL }}
        bazel_remote_username: ${{ secrets.REMOTE_CACHE_USERNAME }}
        bazel_remote_password: ${{ secrets.REMOTE_CACHE_PASSWORD }}
        link_threads: ${{ steps.calculate-threads.outputs.link_threads }}
        test_threads: ${{ steps.calculate-threads.outputs.test_threads }}
        sync_to_s3: ${{ vars.SYNC_TO_S3 }}
        upload_ya_dir: ${{ inputs.upload_ya_dir }}
        clean_ya_dir: ${{ inputs.run_build && 'no' || inputs.clean_ya_dir }}
        use_network_cache: ${{ inputs.use_network_cache }}
        number_of_retries: ${{ inputs.number_of_retries }}
        truncate_enabled: ${{ inputs.truncate_enabled }}
    - id: sleep
      name: Sleep after failed tests
      if: ${{ inputs.sleep_after_tests != '0' && failure() }}
      shell: bash --noprofile --norc -eo pipefail -x {0}
      env:
        SLEEP_AFTER_TESTS: ${{ inputs.sleep_after_tests || '7200' }}
      run: |
        sleep "$SLEEP_AFTER_TESTS"
  debug:
    name: Debug
    runs-on: [self-hosted, "self-hosted", "runner_light"]
    needs: 
      - calculate-runs-on
      - build-and-test
    if: ${{ !cancelled() }}
    steps:
      - name: Debug info
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: |
          echo "Test run outcome: ${{ needs.build-and-test.outputs.test_run_outcome }}"
          echo "Test failed: ${{ needs.build-and-test.outputs.test_failed }}"
          echo "Test run outcome is null: ${{ needs.build-and-test.outputs.test_run_outcome == null }}"
          echo "Test failed is null: ${{ needs.build-and-test.outputs.test_failed == null }}"
          echo "Test run outcome is failure: ${{ needs.build-and-test.outputs.test_run_outcome == 'failure' }}"
          echo "Test failed is no: ${{ needs.build-and-test.outputs.test_failed == 'no' }}"
          echo "Test failed is yes: ${{ needs.build-and-test.outputs.test_failed == 'yes' }}"
          echo "Test run outcome is success: ${{ needs.build-and-test.outputs.test_run_outcome == 'success' }}"
          echo "Test run outcome is skipped: ${{ needs.build-and-test.outputs.test_run_outcome == 'skipped' }}"
          echo "Test run outcome is cancelled: ${{ needs.build-and-test.outputs.test_run_outcome == 'cancelled' }}"
          echo "Test run outcome is empty: ${{ needs.build-and-test.outputs.test_run_outcome == '' }}"
          echo "Test failed is empty: ${{ needs.build-and-test.outputs.test_failed == '' }}"
          echo "Condition: ${{ needs.build-and-test.outputs.test_failed == null && needs.build-and-test.outputs.test_run_outcome == null }}"
          echo "Condition: ${{ needs.build-and-test.outputs.test_failed == '' && needs.build-and-test.outputs.test_run_outcome == '' }}"
          
        env:
          test_run_outcome: ${{ needs.build-and-test.outputs.test_run_outcome }}
          test_failed: ${{ needs.build-and-test.outputs.test_failed }}
          test_run_outcome_is_null: ${{ needs.build-and-test.outputs.test_run_outcome == null }}
          test_failed_is_null: ${{ needs.build-and-test.outputs.test_failed == null }}
          test_run_outcome_is_failure: ${{ needs.build-and-test.outputs.test_run_outcome == 'failure' }}
          test_failed_is_no: ${{ needs.build-and-test.outputs.test_failed == 'no' }}
          test_failed_is_yes: ${{ needs.build-and-test.outputs.test_failed == 'yes' }}
          test_run_outcome_is_success: ${{ needs.build-and-test.outputs.test_run_outcome == 'success' }}
          test_run_outcome_is_skipped: ${{ needs.build-and-test.outputs.test_run_outcome == 'skipped' }}
          test_run_outcome_is_cancelled: ${{ needs.build-and-test.outputs.test_run_outcome == 'cancelled' }}
          test_run_outcome_is_empty: ${{ needs.build-and-test.outputs.test_run_outcome == '' }}
          test_failed_is_empty: ${{ needs.build-and-test.outputs.test_failed == '' }}
    
  test-retry:
    name: Retry tests
    runs-on: ${{ fromJson(needs.calculate-runs-on.outputs.runs-on) }}
    needs: 
      - calculate-runs-on
      - build-and-test
    # we can only rely on fact that test_failed can be null or 
    if: ${{ failure() && (needs.build-and-test.outputs.test_failed == null && needs.build-and-test.outputs.test_run_outcome == null) }}
    steps:
      - name: Prepare workspace
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: |
          chown -R root:root $GITHUB_WORKSPACE
      - name: Checkout PR
        uses: actions/checkout@v4
        if: github.event.pull_request.head.sha != ''
        with:
          submodules: true
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: ${{ !contains(github.event.pull_request.labels.*.name, 'rebase') && 1 || 0 }}
      - name: Rebase PR
        if: ${{ github.event.pull_request.head.sha != '' && contains(github.event.pull_request.labels.*.name, 'rebase') }}
        shell: bash
        run: |
          git config user.email "robot-nbs@nebius.com"
          git config user.name "Robot NBS"
          git fetch origin ${{ github.event.pull_request.base.ref }}
          git rebase origin/${{ github.event.pull_request.base.ref }}
      - name: Checkout
        uses: actions/checkout@v4
        if: github.event.pull_request.head.sha == ''
        with:
          submodules: true
          set-safe-directory: '*'

      - name: set up nebius cli
        id: cli
        uses: ./.github/actions/nebius_cli
        with:
          sa_json: ${{ secrets.NEW_NEBIUS_SA_JSON_CREDENTIALS }}
      - name: set up gh cli
        id: gh
        uses: ./.github/actions/gh_cli
      - name: get runner info
        id: get-runner
        shell: bash --noprofile --norc -eo pipefail -x {0}
        env:
          RUNNER_NAME: ${{ runner.name }}
        run: |
          nebius compute instance get --id "${RUNNER_NAME}" --format json > instance.json
          LABEL=$(jq -r '.metadata.labels."runner-label"' instance.json)
          RUNNER_FLAVOR=$(jq -r '.metadata.labels."runner-flavor"' instance.json)
          EXTERNAL_IPV4=$(jq -r '.status.network_interfaces[0].public_ip_address.address' instance.json | sed -e 's/\/32//g')
          LOCAL_IPV4=$(jq -r '.status.network_interfaces[0].ip_address.address' instance.json | sed -e 's/\/32//g')
          VM_PRESET=$(jq -r '.spec.resources.preset' instance.json)
          {
            echo "LABEL=${LABEL}"
            echo "RUNNER_FLAVOR=${RUNNER_FLAVOR}"
            echo "INSTANCE_ID=${INSTANCE_ID}"
            echo "RUNNER_IPV4=${EXTERNAL_IPV4}"
            echo "RUNNER_LOCAL_IPV4=${LOCAL_IPV4}"
            echo "VM_PRESET=${VM_PRESET}"
          } | tee -a "$GITHUB_OUTPUT"

      - name: Calculate threads
        id: calculate-threads
        if: always()
        uses: ./.github/actions/nebius_threads_calculator
        with:
          vm_preset: ${{ steps.get-runner.outputs.VM_PRESET }}
          tests_size: ${{ inputs.test_size }}

      - name: Prepare s3cmd
        uses: ./.github/actions/s3cmd
        with:
          s3_bucket: ${{ vars.AWS_BUCKET }}
          s3_endpoint: ${{ vars.NEBIUS_AWS_ENDPOINT }}
          s3_website_suffix: ${{ vars.NEBIUS_AWS_WEBSITE_SUFFIX }}
          s3_key_id: ${{ secrets.NEBIUS_AWS_ACCESS_KEY_ID }}
          s3_key_secret: ${{ secrets.NEBIUS_AWS_SECRET_ACCESS_KEY }}
          folder_prefix: nebius-
          build_preset: ${{ inputs.build_preset }}
          install: false
      - name: Run tests
        uses: ./.github/actions/test
        id: test
        with:
          test_target: ${{ inputs.test_target }}
          build_preset: ${{ inputs.build_preset }}
          test_size: ${{ inputs.test_size }}
          test_type: ${{ inputs.test_type }}
          bazel_remote_uri: ${{ vars.NEBIUS_BAZEL_REMOTE_CACHE_URL }}
          bazel_remote_username: ${{ secrets.REMOTE_CACHE_USERNAME }}
          bazel_remote_password: ${{ secrets.REMOTE_CACHE_PASSWORD }}
          link_threads: ${{ steps.calculate-threads.outputs.link_threads }}
          test_threads: ${{ steps.calculate-threads.outputs.test_threads }}
          sync_to_s3: ${{ vars.SYNC_TO_S3 }}
          upload_ya_dir: ${{ inputs.upload_ya_dir }}
          clean_ya_dir: ${{ inputs.clean_ya_dir }}
          use_network_cache: ${{ inputs.use_network_cache }}
          number_of_retries: ${{ inputs.number_of_retries }}
